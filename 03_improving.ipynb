{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Search Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Inspired by and borrowed heavily from: Collective Intelligence - [Luís F. Simões](mailto:luis.simoes@vu.nl). IR version and assignments by J.E. Hoeksema, 2014-11-12. Converted to Python 3 and minor changes by Tobias Kuhn, 2015-11-10 and 2016-10-21.)_\n",
    "\n",
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT: This is not the final version! The assignments will be changed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook's purpose is to improve the search index and query functions built in the previous notebooks and assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data, Defining some functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is copied from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle, bz2, re\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from IPython.display import display, HTML\n",
    "from math import log10\n",
    "\n",
    "Summaries_file = 'data/malaria__Summaries.pkl.bz2'\n",
    "Abstracts_file = 'data/malaria__Abstracts.pkl.bz2'\n",
    "\n",
    "Summaries = pickle.load( bz2.BZ2File( Summaries_file, 'rb' ) )\n",
    "Abstracts = pickle.load( bz2.BZ2File( Abstracts_file, 'rb' ) )\n",
    "\n",
    "paper = namedtuple( 'paper', ['title', 'authors', 'year', 'doi'] )\n",
    "for (id, paper_info) in Summaries.items():\n",
    "    Summaries[id] = paper( *paper_info )\n",
    "    \n",
    "def display_summary( id, extra_text='' ):\n",
    "    \"\"\"\n",
    "    Function for printing a paper's summary through IPython's Rich Display System.\n",
    "    Trims long titles or author lists, and links to the paper's  DOI (when available).\n",
    "    \"\"\"\n",
    "    s = Summaries[ id ]\n",
    "    \n",
    "    title = ( s.title if s.title[-1]!='.' else s.title[:-1] )\n",
    "    title = title[:150].rstrip() + ('' if len(title)<=150 else '...')\n",
    "    if s.doi!='':\n",
    "        title = '<a href=http://dx.doi.org/%s>%s</a>' % (s.doi, title)\n",
    "    \n",
    "    authors = ', '.join( s.authors[:5] ) + ('' if len(s.authors)<=5 else ', ...')\n",
    "    \n",
    "    lines = [\n",
    "        title,\n",
    "        authors,\n",
    "        str(s.year),\n",
    "        '<small>id: %d%s</small>' % (id, extra_text)\n",
    "        ]\n",
    "    \n",
    "    display( HTML( '<blockquote>%s</blockquote>' % '<br>'.join(lines) ) )\n",
    "    \n",
    "def display_abstract( id, highlights=[]):\n",
    "    \"\"\"\n",
    "    Function for displaying an abstract. Includes optional (naive) highlighting\n",
    "    \"\"\"\n",
    "    a = Abstracts[ id ]\n",
    "    for h in highlights:\n",
    "        a = re.sub(r'\\b(%s)\\b'%h,'<mark>\\\\1</mark>',a, flags=re.IGNORECASE)\n",
    "    display( HTML( '<blockquote>%s</blockquote' % a ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function that tokenizes a string in a rather naive way. Can be extended later.\n",
    "    \"\"\"\n",
    "    return text.split(' ')\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\"\n",
    "    Perform linguistic preprocessing on a list of tokens. Can be extended later.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.append(token.lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inverted_index = defaultdict(set)\n",
    "\n",
    "for (id, abstract) in Abstracts.items():\n",
    "    for term in preprocess(tokenize(abstract)):\n",
    "        inverted_index[term].add(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see from the results of the last assignment, our simple index doesn't handle punctuation and the difference between singular and plural versions of the same word very well. A possible solution to those issues would be to apply better tokenization and stemming. Fortunately, Python's _NLTK_ package provides implementations of these algorithms we can use. You have to install _NLTK_ by following [these instructions](http://www.nltk.org/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tk/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "['Good', 'muffins', 'cost', '$3.88\\nin', 'New', 'York.', '', 'Please', 'buy', 'me', 'two', 'of', 'them.\\n\\nThanks.']\n",
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n",
    "\n",
    "print(tokenize(s))\n",
    "print(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"processes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to improve our search results is to *rank* them. A possible way to do this is to calculate a score for each document based on the matching terms from the query. One such scoring method is *tf-idf*, as explained in the lecture slides.\n",
    "\n",
    "In order to quickly calculate the scores for a term/document combination, we'll need quick access to a couple of things:\n",
    "* tf(t,d) - How often does a term occur in a document\n",
    "* df(t) - In how many documents does a term occur\n",
    "* N - The number of documents in our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_matrix = defaultdict(Counter)\n",
    "for (id, abstract) in Abstracts.items():\n",
    "    tf_matrix[id] = Counter(preprocess(tokenize(abstract)))\n",
    "\n",
    "def tf(t,d):\n",
    "    return float(tf_matrix[d][t])\n",
    "\n",
    "def df(t):\n",
    "    return float(len(inverted_index[t]))\n",
    "    \n",
    "def num_documents():\n",
    "    return float(len(Abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "403.0\n",
      "67028.0\n"
     ]
    }
   ],
   "source": [
    "print(tf('network',24130474))\n",
    "print(df('network'))\n",
    "print(num_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these three helper functions, we can now easily calculate the _tf-idf_ weights of a term in a document by implementing the weighting formula from the slides, which you will do in the assignments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Change (in the code cell below) the *smarter_tokenize* function to use NLTK's word_tokenize function for tokenization, and the *smarter_preprocess* function to perform stemming in addition to case normalization. Does `smarter_and_query(\"air sample\")` return the paper *26488732*? Why (not)?\n",
    "\n",
    "  _Note:_ We are generating this index on a subset of the data, as generating an index with stemming on the entire set would take up to half an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change this code according to the task above:\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def smarter_tokenize(text):\n",
    "    # Change this\n",
    "    return text.split(' ')\n",
    "\n",
    "def smarter_preprocess(tokens):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        # Change this\n",
    "        result.append(token.lower())\n",
    "    return result\n",
    "\n",
    "def smarter_and_query(query): # Regular and_query using smarter_tokenize and smarter_preprocess\n",
    "    return reduce(lambda a, e: a.intersection(e), [smarter_index[term] for term in smarter_preprocess(smarter_tokenize(query))])\n",
    "\n",
    "smarter_index = defaultdict(set)\n",
    "# The code below creates an inverted index based on a subset of the documents\n",
    "subset = set(Abstracts.keys()).intersection(set(range(26400000,26500000)))\n",
    "\n",
    "for (id, abstract) in ((k, Abstracts[k]) for k in subset):\n",
    "    for term in smarter_preprocess(smarter_tokenize(abstract)):\n",
    "        smarter_index[term].add(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Write your answer text here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a function `tfidf(t,d)` that returns the tf.idf score of term `t` in document `d` by using the `tf(t,d)`, `df(t)` and `num_documents()` functions we defined above. The _tf-idf_ formula can be found on the lecture slides. Test your function with the examples shown below.\n",
    "  \n",
    "  You can use our old index for this task and the tasks below: You do **not** need to include the results from above with the smarter tokenization and preprocessing functions.\n",
    "  \n",
    "  You can use the `log10(n)` function to calculate log<sub>10</sub>(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "\n",
    "#print(tfidf('air', 26488732))\n",
    "#print(tfidf('samples', 26488732))\n",
    "#print(tfidf('monkey', 26488732))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a function `query(query_string)`, which accepts as input a single query string that could consist of one or more words, and returns or prints a list of (up to) 10 best matching documents, along with their score. \n",
    "\n",
    "  You should use _tf-idf_ to calculate document scores based on the query, and the results should be ordered by score in descending order.\n",
    "\n",
    "  _Hint:_ Start by copying your `or_query` function from mini-assignment 2, then expand that to rank the results, making use of the `tfidf(t,d)` function you created earlier.\n",
    "\n",
    "  Use the provided `display_summary(id,extra_text)` function to make the output a bit more 'search engine'-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Come up with a few example queries to run, and include the output here. Do the results make sense? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Write your answer text here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
